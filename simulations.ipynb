{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mixGauss(n, mu_1 = 0, mu_2 = 2, sig_1 = 1, sig_2 = 1, alpha = 0.2, set_seed = None, form= 'mix-norm'):\n",
    "    \"\"\" Generate mixture of two 1D-Gaussians with given parameters\"\"\"\n",
    "    X = np.zeros(n)\n",
    "    assert (0 <= alpha <= 1), \"alpha proportion outside of [0,1] range\"\n",
    "\n",
    "    if isinstance(set_seed, int):\n",
    "        np.random.seed(set_seed)\n",
    "    \n",
    "    if form == 'mix-norm':\n",
    "        for i in range(n):\n",
    "            draw = np.random.uniform()\n",
    "            if draw < alpha:\n",
    "                X[i] = np.random.normal(mu_1, sig_1)\n",
    "            else:\n",
    "                X[i] = np.random.normal(mu_2, sig_2)\n",
    "\n",
    "    return X\n",
    "\n",
    "def GaussianPrior(x, loc = 1, sigma = 1, lambd = 0.1):\n",
    "    return stats.norm(loc, np.sqrt((sigma**2)/lambd)).pdf(x)\n",
    "\n",
    "def GaussianLlhood(x, mu, sigma = 1):\n",
    "    return stats.norm(mu, sigma).pdf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianMixturePMC(X, N, V, T, n_parameters = 2, start_values = None, baseline_threshold = 0.01, alpha = 0.5):\n",
    "    \"\"\" PMC Sampler for a simple Gaussian Mixture \n",
    "\n",
    "    X = Observations vector\n",
    "    N = length of each set of sampled values at each iteration (fixed in this implementation across iterations)\n",
    "    V = Variances vector\n",
    "    T = number of iterations\n",
    "    n_parameters = number of parameters(means) to be estimated\n",
    "    start_values = (N x n_parameters) matrix, if None: take initial values of means as the means of the observations X, otherwise starting means\n",
    "    baseline_threshold = percentage of generated samples at each iteration to be kept for each variance level\n",
    "    alpha = mixture proportions\n",
    "    \"\"\"\n",
    "    p = len(V)\n",
    "\n",
    "    baseline = int(N*baseline_threshold*p) #Used to avoid certain variances from vanishing\n",
    "\n",
    "    assert (N%p == 0), \"N%p is not divisible\" \n",
    "    R = [Counter(V)]\n",
    "\n",
    "    if start_values is None:\n",
    "        mu0 = np.full((N, n_parameters+1), np.mean(X))\n",
    "        mu0[:, -1] = [i for sublist in [[x]*int(N/p) for x in V] for i in sublist]\n",
    "        mu0_base = np.full((baseline, n_parameters+1), np.mean(X))\n",
    "        mu0_base[:, -1] = [i for sublist in [[x]*int(baseline/p) for x in V] for i in sublist]\n",
    "        mu0 = np.concatenate([mu0, mu0_base])\n",
    "    else:\n",
    "        # start_values = np.array(start_values)\n",
    "        # if start_values.shape == (n_parameters,):\n",
    "        #     mu0 = np.tile(start_values, (N+baseline, 1))\n",
    "        #     m\n",
    "\n",
    "        #np.assert(start_values.shape == (N+baseline, n_parameters+1)\n",
    "        mu0 = start_values\n",
    "\n",
    "    mu = mu0\n",
    "    mus = [mu0]\n",
    "\n",
    "    W = np.zeros(N + baseline)\n",
    "    for t in range(T):\n",
    "        if t%10 == 0:\n",
    "            print('iteration: ', t)\n",
    "        mu_star = mu.copy()\n",
    "        for n in range(N+baseline):\n",
    "            for i in range(len(mu[n]) - 1):\n",
    "                #Perform isotropic perturbation of previous means batch, each sample with assigned variance\n",
    "                mu_star[n][i] = np.random.normal(mu[n][i], np.sqrt(mu[n][-1]))\n",
    "\n",
    "            #Assign each weight \n",
    "            W[n] = np.sum(np.log(GaussianLlhood(X, mu_star[n][0], sigma = np.sqrt(1))*alpha + GaussianLlhood(X, mu_star[n][1], sigma = np.sqrt(1))*(1-alpha))) +\\\n",
    "            np.log(GaussianPrior(mu_star[n][0])*GaussianPrior(mu_star[n][1])) -\\\n",
    "            (np.log(GaussianLlhood(mu_star[n][0], mu = mu[n][0], sigma = np.sqrt(mu[n][-1]))) + \\\n",
    "            np.log(GaussianLlhood(mu_star[n][1], mu = mu[n][1], sigma = np.sqrt(mu[n][-1]))))\n",
    "\n",
    "        P = np.exp(W-np.max(W))\n",
    "        weights_normalized = P/np.sum(P)   \n",
    "\n",
    "        resample_idx = np.random.choice(N+baseline, N+baseline, p  = weights_normalized)\n",
    "        mu = np.array([mu_star[x] for x in resample_idx])\n",
    "\n",
    "        mu[-baseline:, -1] = [i for sublist in [[x]*int(baseline/p) for x in V] for i in sublist] #TO UPDATE IN FINAL VERSION CAREFUL!!!!\n",
    "        V_assignments = mu[:, -1].copy()\n",
    "        R.append(Counter(V_assignments))\n",
    "        mus.append(mu.copy())\n",
    "        #Reshuffle Variances\n",
    "        np.random.shuffle(V_assignments)\n",
    "        mu[:, -1] = V_assignments\n",
    "\n",
    "    return mus, pd.DataFrame(R)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8040174adb352a73135c8446650f1cc32361abd9ba926cc925dae6e4ab16e8ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
